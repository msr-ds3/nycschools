---
title: "20170724_predicting_continuation_take_2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Automated Guidance Counselor
##Predict if a student will continue to the next grade.

```{r cars}
library(dplyr)
library(tidyr)
library(ggplot2)
library(reshape2)
library(boot)
library(glmnet)
library(tidyverse)
library(scales)
library(ROCR)

files <- Sys.glob('/data/nycdoe/June Biog/*.csv')

read_csv_with_year <- function(filename) {
  year <- as.numeric(gsub('-.*$', '', basename(filename)))
  df <- read_csv(filename, col_types = cols(grade_level=col_character(), grade_code=col_character(), admit_code=col_character(), adcode=col_character(),fall_days_present = col_character(), spring_days_present = col_character()))
  df$year <- year
  df
}

bios1 <- map_df(files, read_csv_with_year)
```

Tidy up June Bios data for modeling.
```{r}
bios2 <-
  bios1 %>%
  filter(grade_level!="AD", grade_level!="IN") %>%
  mutate(grade_level=ifelse(grade_level=="0K", 0, 
                            ifelse(grade_level=="PK", -1, 
                                   as.numeric(grade_level)))) %>%
  mutate(disability=ifelse(is.na(disability), 'ND', disability)) %>%
  mutate(perc_attendance=
           ifelse(is.na(days_abs),
                  (as.numeric(fall_days_present)+as.numeric(spring_days_present))/(fall_days_absent+spring_days_absent+as.numeric(fall_days_present)+as.numeric(spring_days_present)),
                                (days_pres+days_released)/(days_pres+days_abs+days_released)))

bios2 %>% 
  summarise(p=sum(is.na(perc_attendance))/n())
```

7% of our attendances are missing. For now, we are going to drop those rows. Later, we'll try bucketing attendance?

```{r}
load('/data/nycdoe/clean_data/tidy_grad_data.Rdata')
bios3 <-
  bios2 %>%
  filter(!is.na(perc_attendance)) 

bios4 <-
  bios3 %>%
  group_by(student_id_scram) %>%
  arrange(year) %>%
  mutate(did_continue=ifelse(!is.na(lead(grade_level)) & grade_level == lead(grade_level) - 1, 1, 0)) %>%
  ungroup() %>%
  mutate(grade_level=as.character(grade_level), year=as.character(year))

bios5 <-
  bios4 %>%
  left_join(tidy_grad, by=c('student_id_scram'))

bios6 <-
  bios5 %>%
  mutate(did_continue=ifelse(grade_level=='12', graduated, did_continue))
```

```{r}
bios6 %>% 
  group_by(grade_level) %>% 
  summarise(num_nas=sum(is.na(did_continue)), perc_nas=sum(is.na(did_continue))/n())

bios7 <-
  bios6 %>%
  group_by(student_id_scram) %>%
  filter(!any(is.na(did_continue))) %>%
  ungroup()
```

3% of 12th graders in June Bios are not listed in Grad data, so we do not know what to set the bit for them in `did_continue`. For now, we are dropping those students.

```{r}
bios8 <-
  bios7 %>%
  mutate(did_not_continue=!did_continue)

bios9 <-
  bios8 %>%
  select(did_continue, everything()) %>%
  filter(year %in% c("2005", "2006", "2007", "2008", "2009", "2010", "2011"))

rm(bios1, bios2, bios3, bios4, bios5, bios6, bios7)
```
Note: 2012-2015 don't have the disability disability column - dropping those years for now.

Create test and train sets.
```{r}
sample_size <- floor(0.80 * nrow(bios9))

set.seed(18)
train_ind <- sample(seq_len(nrow(bios9)), size=sample_size)

train <- bios9[train_ind, ]
test <- bios9[-train_ind, ]
```

Model #2.
```{r}
train_with_features <-
  train %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, disability, poverty, perc_attendance) 
model_data <- train_with_features %>%
  group_by_at(setdiff(names(train_with_features), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_train <- model.matrix(~ grade_level + sex + ethnicity + disability + ell + poverty + perc_attendance, data=model_data)
y_train <- cbind(model_data$no, model_data$yes)

model_2 <- glmnet(X_train, y_train, family="binomial", lambda = 0)
```

##Evaluating model_2.

Distribution of our predictions.
```{r}
model_data_test <-
  test %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, disability, poverty, perc_attendance) %>%
  group_by_at(setdiff(names(train_with_features), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_test <-
  model.matrix(~ grade_level + sex + ethnicity + disability + ell + poverty + perc_attendance, data=model_data_test)

y_test <-
  cbind(model_data_test$no, model_data_test$yes)

for (col in setdiff(colnames(X_train), colnames(X_test))) {
  m <-matrix(0, nrow=nrow(X_test))
  colnames(m) <- c(col)
  X_test <- cbind(X_test, m)
}

X_test <- (X_test[,colnames(X_train)])

data.frame(p=as.numeric(predict(model_2, X_test, type="response")) ) %>% 
  ggplot(aes(x=p)) + 
  geom_histogram()
```


Calculating accuracy of our model.
```{r}
predictions_model_2 <-
  data.frame(y_test) %>%
  merge(data.frame(predict(model_2, X_test, type="response")), by="row.names")

colnames(predictions_model_2)<-c("Row.names", "cont", "did_not_cont", "p_did_not_continue")

model_2_stats <-
  predictions_model_2 %>%
  mutate(total=cont+did_not_cont, num_correct=did_not_cont*(p_did_not_continue>=0.5) + cont*(p_did_not_continue<0.5))
  
accuracy <-
  model_2_stats %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

```{r}
precision <-
  model_2_stats %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

```{r}
tpr <-
  model_2_stats %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

```{r}
fpr <-
  model_2_stats %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot
```{r}
# predicted = p, actual = yes
model_2_stats %>%
  group_by(predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 2', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students')
```

ROC curve
```{r}
roc_data_1 <- data.frame(matrix(NA, nrow = 1000, ncol = 2))
colnames(roc_data_1) <- c("tpr", "fpr")

for (i in 1:1000) {
  thresh=i/1000
  temp <-
    model_2_stats %>%
    summarise(tpr=sum(did_not_cont*(p_did_not_continue>=thresh))/sum(did_not_cont),
              fpr=sum(cont*(p_did_not_continue>=thresh))/sum(cont))

  roc_data_1[i, 'tpr'] <- temp[1, 1]
  roc_data_1[i, 'fpr'] <- temp[1, 2]
}

roc_data_1 %>%
  ggplot(aes(x=fpr, y=tpr)) +
  geom_line() +
  xlim(0, 1) +
  geom_abline(linetype='dashed')
```

Calculating the AUC
```{r}
auc <-
  model_2_stats %>%
  summarise(auc=sum(p_did_not_continue>=0.5)/sum(total))

auc
```
Is this correct?

## Breaking down metrics by demographics

1. By ethnicity

```{r}
X_test_new <-
  data.frame(X_test) %>%
  mutate(Row.names=row_number()) 

model_2_stats_with_x <-
  model_2_stats %>%
  mutate(Row.names=as.numeric(Row.names)) %>%
  left_join(X_test_new, by="Row.names")

model_2_stats_ethnic <-
  model_2_stats_with_x %>%
  select(2:6, grep('ethnicity', colnames(.))) %>%
  gather('ethnicity', 'value', 6:11) %>%
  filter(value==1) %>%
  group_by(ethnicity)
```

Distribution of predictions.
```{r}
model_2_stats_ethnic %>%
  ungroup() %>%
  ggplot(aes(x=p_did_not_continue)) + 
  geom_histogram() +
  facet_wrap(~ethnicity)
```

Accuracy.
```{r}
accuracy <-
  model_2_stats_ethnic %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

Precision.
```{r}
precision <-
  model_2_stats_ethnic %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

True positive rate.
```{r}
tpr <-
  model_2_stats_ethnic %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

False positive rate.
```{r}
fpr <-
  model_2_stats_ethnic %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot.
```{r}
model_2_stats_ethnic %>%
  group_by(ethnicity, predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 2', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students') +
  facet_wrap(~ethnicity)
```

2. By gender

```{r}
model_2_stats_gender <-
  model_2_stats_with_x %>%
  select(2:6, grep('sex', colnames(.))) %>%
  group_by(sexM)
```

Distribution of predictions.
```{r}
model_2_stats_gender %>%
  ungroup() %>%
  ggplot(aes(x=p_did_not_continue)) + 
  geom_histogram() +
  facet_wrap(~as.factor(sexM))
```


Accuracy.
```{r}
accuracy <-
  model_2_stats_gender %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

Precision.
```{r}
precision <-
  model_2_stats_gender %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

True positive rate.
```{r}
tpr <-
  model_2_stats_gender %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

False positive rate.
```{r}
fpr <-
  model_2_stats_gender %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot.
```{r}
model_2_stats_gender %>%
  group_by(sexM, predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 2', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students') +
  facet_wrap(~sexM)
```

## Model 3

```{r}
load('/data/nycdoe/clean_data/stud_perf_quantiledGPA.Rdata')

stud_perf_quantGPA <-
  stud_perf_quantGPA %>%
  select(year, student_id_scram, quantiled_GPA) %>%
  mutate(year=as.character(year))

```

```{r}

bios10 <-
  bios9 %>%
  mutate(student_id_scram=as.character(student_id_scram)) %>%
  left_join(stud_perf_quantGPA, by=c('year','student_id_scram')) %>%
  filter(year %in% c("2005", "2006", "2007", "2008", "2009", "2010", "2011"))

sample_size <- floor(0.80 * nrow(bios10))

set.seed(18)
train_ind <- sample(seq_len(nrow(bios10)), size=sample_size)

train_3 <- bios10[train_ind, ]
test_3 <- bios10[-train_ind, ]
 
model_data_3 <- 
  train_3 %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, disability, poverty, perc_attendance, quantiled_GPA) %>%
  group_by_at(setdiff(names(.), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_train <- model.matrix(~ grade_level + sex + ethnicity + disability + ell + poverty + perc_attendance + quantiled_GPA, data=model_data_3)
#X_train <- model.matrix(~ -1 + quantiled_GPA, data=model_data_3)
y_train <- cbind(model_data_3$no, model_data_3$yes)

model_3 <- glmnet(X_train, y_train, family="binomial", lambda = 0)
#model_4 <- glm(cbind(model_data_3$yes, model_data_3$no) ~ quantiled_GPA, family="binomial", data=model_data_3)
```

##Evaluating model_3.

Distribution of our predictions.
```{r}
model_data_test_3 <-
  test_3 %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, disability, poverty, perc_attendance, quantiled_GPA) %>%
  group_by_at(setdiff(names(.), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_test <-
  model.matrix(~ grade_level + sex + ethnicity + disability + ell + poverty + perc_attendance + quantiled_GPA, data=model_data_test_3)

y_test <-
  cbind(model_data_test_3$no, model_data_test_3$yes)

for (col in setdiff(colnames(X_train), colnames(X_test))) {
  m <-matrix(0, nrow=nrow(X_test))
  colnames(m) <- c(col)
  X_test <- cbind(X_test, m)
}

X_test <- (X_test[,colnames(X_train)])

data.frame(p=as.numeric(predict(model_3, X_test, type="response")) ) %>% 
  ggplot(aes(x=p)) + 
  geom_histogram()
```


Calculating accuracy of our model.
```{r}
predictions_model_3 <-
  data.frame(y_test) %>%
  merge(data.frame(predict(model_3, X_test, type="response")), by="row.names")

colnames(predictions_model_3)<-c("Row.names", "cont", "did_not_cont", "p_did_not_continue")

model_3_stats <-
  predictions_model_3 %>%
  mutate(total=cont+did_not_cont, num_correct=did_not_cont*(p_did_not_continue>=0.5) + cont*(p_did_not_continue<0.5))
  
accuracy <-
  model_3_stats %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

```{r}
precision <-
  model_3_stats %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

```{r}
tpr <-
  model_3_stats %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

```{r}
fpr <-
  model_3_stats %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot
```{r}
# predicted = p, actual = yes
model_3_stats %>%
  group_by(predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 2', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students')
```

ROC curve
```{r}
roc_data_2 <- data.frame(matrix(NA, nrow = 1000, ncol = 2))
colnames(roc_data_2) <- c("tpr", "fpr")

for (i in 1:1000) {
  thresh=i/1000
  temp <-
    model_3_stats %>%
    summarise(tpr=sum(did_not_cont*(p_did_not_continue>=thresh))/sum(did_not_cont),
              fpr=sum(cont*(p_did_not_continue>=thresh))/sum(cont))

  roc_data_2[i, 'tpr'] <- temp[1, 1]
  roc_data_2[i, 'fpr'] <- temp[1, 2]
}

roc_data_2 %>%
  ggplot(aes(x=fpr, y=tpr)) +
  geom_line() +
  xlim(0, 1) +
  geom_abline(linetype='dashed')
```

## 20170725

```{r}
bios11 <-
  bios9 %>%
  mutate(student_id_scram=as.character(student_id_scram)) %>%
  left_join(stud_perf_quantGPA, by=c('year','student_id_scram'))

sample_size <- floor(0.80 * nrow(bios11))

set.seed(19)
train_ind <- sample(seq_len(nrow(bios11)), size=sample_size)

train_4 <- bios11[train_ind, ]
test_4 <- bios11[-train_ind, ]
 
model_data_4 <- 
  train_4 %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, poverty, perc_attendance, quantiled_GPA) %>%
  group_by_at(setdiff(names(.), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_train <- model.matrix(~ grade_level + sex + ethnicity + ell + poverty + perc_attendance + quantiled_GPA, data=model_data_4)
#X_train <- model.matrix(~ -1 + quantiled_GPA, data=model_data_3)
y_train <- cbind(model_data_4$no, model_data_4$yes)

model_4 <- glmnet(X_train, y_train, family="binomial", lambda = 0)
```

##Evaluating model_4 - removed disability.

Distribution of our predictions.
```{r}
model_data_test_4 <-
  test_4 %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, poverty, perc_attendance, quantiled_GPA) %>%
  group_by_at(setdiff(names(.), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_test <-
  model.matrix(~ grade_level + sex + ethnicity + ell + poverty + perc_attendance + quantiled_GPA, data=model_data_test_4)

y_test <-
  cbind(model_data_test_4$no, model_data_test_4$yes)

for (col in setdiff(colnames(X_train), colnames(X_test))) {
  m <-matrix(0, nrow=nrow(X_test))
  colnames(m) <- c(col)
  X_test <- cbind(X_test, m)
}

X_test <- (X_test[,colnames(X_train)])

data.frame(p=as.numeric(predict(model_4, X_test, type="response")) ) %>% 
  ggplot(aes(x=p)) + 
  geom_histogram()
```


Calculating accuracy of our model.
```{r}
predictions_model_4 <-
  data.frame(y_test) %>%
  merge(data.frame(predict(model_4, X_test, type="response")), by="row.names")

colnames(predictions_model_4)<-c("Row.names", "cont", "did_not_cont", "p_did_not_continue")

model_4_stats <-
  predictions_model_4 %>%
  mutate(total=cont+did_not_cont, num_correct=did_not_cont*(p_did_not_continue>=0.5) + cont*(p_did_not_continue<0.5))
  
accuracy <-
  model_4_stats %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

```{r}
precision <-
  model_4_stats %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

```{r}
tpr <-
  model_4_stats %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

```{r}
fpr <-
  model_4_stats %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot
```{r}
# predicted = p, actual = yes
model_4_stats %>%
  group_by(predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 2', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students')
```

ROC curve
```{r}
roc_data_3 <- data.frame(matrix(NA, nrow = 1000, ncol = 2))
colnames(roc_data_3) <- c("tpr", "fpr")

for (i in 1:1000) {
  thresh=i/1000
  temp <-
    model_4_stats %>%
    summarise(tpr=sum(did_not_cont*(p_did_not_continue>=thresh))/sum(did_not_cont),
              fpr=sum(cont*(p_did_not_continue>=thresh))/sum(cont))

  roc_data_3[i, 'tpr'] <- temp[1, 1]
  roc_data_3[i, 'fpr'] <- temp[1, 2]
}

roc_data_3 %>%
  ggplot(aes(x=fpr, y=tpr)) +
  geom_line() +
  xlim(0, 1) +
  geom_abline(linetype='dashed')
```

AUC and ROC for Model 4 using ROCR package
```{r}
auc_data <-
  model_4_stats %>%
  select(cont, did_not_cont, p_did_not_continue) %>%
  gather('label', 'count', 1:2) %>%
  mutate(label=ifelse(label=='cont', 0, 1)) %>%
  mutate(actual_did_not_continue=label) %>%
  select(actual_did_not_continue, p_did_not_continue, count)

auc_data <- auc_data[rep(row.names(auc_data), auc_data$count), 1:2]

pred <- prediction(auc_data$p_did_not_continue, auc_data$actual_did_not_continue)

perf_nb <- performance(pred, measure='tpr', x.measure='fpr')

plot(perf_nb)

performance(pred, 'auc')
```

## Break up model_4 stats by ethnicity

```{r}
model_4_stats_with_x <-
  model_4_stats %>%
  mutate(Row.names=as.numeric(Row.names)) %>%
  left_join(X_test_new, by="Row.names")

model_4_stats_ethnic <-
  model_4_stats_with_x %>%
  select(2:6, grep('ethnicity', colnames(.))) %>%
  gather('ethnicity', 'value', 6:11) %>%
  filter(value==1) %>%
  group_by(ethnicity)
```

Distribution of predictions.
```{r}
model_4_stats_ethnic %>%
  ungroup() %>%
  ggplot(aes(x=p_did_not_continue)) + 
  geom_histogram() +
  facet_wrap(~ethnicity)
```

Accuracy.
```{r}
accuracy <-
  model_4_stats_ethnic %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

Precision.
```{r}
precision <-
  model_4_stats_ethnic %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

True positive rate.
```{r}
tpr <-
  model_4_stats_ethnic %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

False positive rate.
```{r}
fpr <-
  model_4_stats_ethnic %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot.
```{r}
model_4_stats_ethnic %>%
  group_by(ethnicity, predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 2', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students') +
  facet_wrap(~ethnicity)
```

## Joining in school data.
```{r}
load('/data/nycdoe/clean_data/att_rate_per_school_with_year.Rdata')

SchoolPresRate <-
  SchoolPresRate %>%
  select(year, dbn, presentRate) %>%
  mutate(year=as.character(year), school_avg_attendance=presentRate)

bios12 <-
  bios8 %>%
  left_join(SchoolPresRate, by=c('year','dbn')) %>%
  filter(year %in% c("2005", "2006", "2007", "2008", "2009", "2010", "2011"))

load('/data/nycdoe/clean_data/percent_grad_per_school.Rdata')

percent_grad <-
  percent_grad %>%
  select(dbn, percentGrad) %>%
  mutate(school_avg_grad=as.character(round((percentGrad/10))*10))

bios13 <-
  bios12 %>%
  left_join(percent_grad, by='dbn')

load('/data/nycdoe/clean_data/regent_avgs_with_count.Rdata')

regent_avgs_with_count <-
  regent_avgs_with_count %>%
  group_by(dbn, main_exams) %>%
  summarise(avg_score=mean(avgGrade)) %>%
  mutate(avg_score=as.character(round((avg_score/10))*10)) %>%
  spread(main_exams, avg_score)

regent_avgs_with_count[is.na(regent_avgs_with_count)] <- 'Missing'
colnames(regent_avgs_with_count)[2:6] <- paste("school_avg_", colnames(regent_avgs_with_count[,c(2:6)]), sep = "")

bios14 <-
  bios13 %>%
  left_join(regent_avgs_with_count, by='dbn') %>%
  mutate(student_id_scram=as.character(student_id_scram)) %>%
  left_join(stud_perf_quantGPA, by=c('year','student_id_scram')) %>%
  mutate(perc_attendance=as.character(round((perc_attendance*10))*10),
school_avg_attendance=as.character(round((school_avg_attendance*10))*10))

bios15 <-
  bios14 %>%
  select(did_not_continue, grade_level, sex, ethnicity, home_lang, pob_code, ell, swd, poverty, perc_attendance, quantiled_GPA, dbn, school_avg_attendance, school_avg_grad, school_avg_English, school_avg_History, school_avg_Language, school_avg_Math, school_avg_Science)

bios15[is.na(bios15)] <- 'Not Applicable'

sample_size <- floor(0.80 * nrow(bios15))

set.seed(19)
train_ind <- sample(seq_len(nrow(bios15)), size=sample_size)

train_5 <- bios15[train_ind, ]
test_5 <- bios15[-train_ind, ]
 
model_data_5 <- 
  train_5 %>%
  group_by_at(setdiff(names(.), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_train <- model.matrix(~ grade_level + sex + ethnicity + ell + poverty + perc_attendance + quantiled_GPA + school_avg_attendance + school_avg_grad + school_avg_English + school_avg_History + school_avg_Language + school_avg_Math + school_avg_Science, data=model_data_5)
y_train <- cbind(model_data_5$no, model_data_5$yes)

model_5 <- glmnet(X_train, y_train, family="binomial", lambda = 0)
```

##Evaluating model_5.

Distribution of our predictions.
```{r}
model_data_test_5 <-
  test_5 %>%
  group_by_at(setdiff(names(.), "did_not_continue")) %>%
  summarize(yes = sum(did_not_continue), no = n() - sum(did_not_continue))

X_test <-
  model.matrix(~ grade_level + sex + ethnicity + ell + poverty + perc_attendance + quantiled_GPA + school_avg_attendance + school_avg_grad + school_avg_English + school_avg_History + school_avg_Language + school_avg_Math + school_avg_Science, data=model_data_test_5)

y_test <-
  cbind(model_data_test_5$no, model_data_test_5$yes)

for (col in setdiff(colnames(X_train), colnames(X_test))) {
  m <-matrix(0, nrow=nrow(X_test))
  colnames(m) <- c(col)
  X_test <- cbind(X_test, m)
}

X_test <- (X_test[,colnames(X_train)])

data.frame(p=as.numeric(predict(model_5, X_test, type="response")) ) %>% 
  ggplot(aes(x=p)) + 
  geom_histogram()
```


Calculating accuracy of our model.
```{r}
predictions_model_5 <-
  data.frame(y_test) %>%
  merge(data.frame(predict(model_5, X_test, type="response")), by="row.names")

colnames(predictions_model_5)<-c("Row.names", "cont", "did_not_cont", "p_did_not_continue")

model_5_stats <-
  predictions_model_5 %>%
  mutate(total=cont+did_not_cont, num_correct=did_not_cont*(p_did_not_continue>=0.5) + cont*(p_did_not_continue<0.5))
  
accuracy <-
  model_5_stats %>%
  summarise(accuracy=sum(num_correct)/sum(total), baseline_accuracy=sum(cont)/sum(total))

accuracy
```

```{r}
precision <-
  model_5_stats %>%
  filter(p_did_not_continue>=0.5) %>%
  summarise(prec=sum(cont)/sum(total))

precision
```

```{r}
tpr <-
  model_5_stats %>%
  summarise(tpr=sum(did_not_cont*(p_did_not_continue>=0.5))/sum(did_not_cont))

tpr
```

```{r}
fpr <-
  model_5_stats %>%
  summarise(fpr=sum(cont*(p_did_not_continue>=0.5))/sum(cont))

fpr
```

Calibration plot
```{r}
# predicted = p, actual = yes
model_5_stats %>%
  group_by(predicted=round(p_did_not_continue*10)/10) %>%
  summarise(num=sum(total), actual=sum(did_not_cont)/sum(total)) %>%
  ggplot(aes(x=predicted, y=actual, size=num)) +
  geom_point() +
  geom_abline(linetype=2) +
  scale_x_continuous(labels=percent, lim=c(0,1)) +
  scale_y_continuous(labels=percent, lim=c(0,1)) +
  scale_size_continuous(labels=comma) +
  labs(title='Calibration plot for Model 5', x='Predicted probability that student will not continue', y= 'Percent of students that actually did not continue', size='Number of students')
```

ROC curve
```{r}
roc_data_5 <- data.frame(matrix(NA, nrow = 1000, ncol = 2))
colnames(roc_data_5) <- c("tpr", "fpr")

for (i in 1:1000) {
  thresh=i/1000
  temp <-
    model_5_stats %>%
    summarise(tpr=sum(did_not_cont*(p_did_not_continue>=thresh))/sum(did_not_cont),
              fpr=sum(cont*(p_did_not_continue>=thresh))/sum(cont))

  roc_data_5[i, 'tpr'] <- temp[1, 1]
  roc_data_5[i, 'fpr'] <- temp[1, 2]
}

roc_data_5 %>%
  ggplot(aes(x=fpr, y=tpr)) +
  geom_line() +
  xlim(0, 1) +
  geom_abline(linetype='dashed')
```

If we can only speak to 100 students and we take the students the model is most confident will drop out, what percentage of students we take will actually drop out?

```{r}
model_5_stats %>%
  arrange(desc(p_did_not_continue)) %>%
  mutate(cum_total=cumsum(total), cum_did_not_cont=cumsum(did_not_cont), perc_cum_did_not_cont=cum_did_not_cont/sum(did_not_cont)) %>%
  ggplot(aes(x=cum_total, y=perc_cum_did_not_cont)) +
  geom_line() +
  scale_x_continuous(labels=comma) +
  scale_y_continuous(labels=percent) +
  labs(x="Number of Students Predicted To Drop Out", y="Percent of Students Caught (Correctly Identified As Dropping Out)")
```
```{r}
b <- data.frame(c(1,1))
b[1,1] <-
  model_5_stats %>%
  summarise(sum(did_not_cont)/sum(total))


model_5_stats %>%
  arrange(desc(p_did_not_continue)) %>%
  mutate(cum_total=cumsum(total), cum_did_not_cont=cumsum(did_not_cont)) %>%
  ggplot(aes(x=cum_total, y=cum_did_not_cont)) +
  geom_line() +
  scale_x_continuous(labels=comma) +
  scale_y_continuous(labels=comma) +
  labs(x="Number of Students Predicted To Drop Out", y="Number of Students Caught (Correctly Identified As Dropping Out)") +
  geom_abline(linetype='dashed', color='blue') +
  geom_abline(slope=b[1,1], intercept=0, linetype='dashed', color='red')
  #geom_abline(aes(data=b), slope=b$c.1..1., intercept=0, linetype='dashed', color=b$c.1..1., show.legend = T) 
```

```{r}
model_5_stats %>%
  arrange(desc(p_did_not_continue)) %>%
  mutate(cum_total=cumsum(total), perc_cum_total=cum_total/sum(total), cum_did_not_cont=cumsum(did_not_cont), perc_cum_did_not_cont=cum_did_not_cont/sum(did_not_cont)) %>%
  ggplot(aes(x=perc_cum_total, y=perc_cum_did_not_cont)) +
  geom_line() +
  scale_x_continuous(labels=percent) +
  scale_y_continuous(labels=percent) +
  labs(x="Percent of Students Predicted To Drop Out", y="Percent of Students Caught (Correctly Identified As Dropping Out)") +
  geom_abline(linetype='dashed', color='red') +
  geom_abline(linetype='dashed', slope=(1/b[1,1]), intercept=0, color='blue')
```

AUC and ROC for Model 5 using ROCR package
```{r}
auc_data <-
  model_5_stats %>%
  select(cont, did_not_cont, p_did_not_continue) %>%
  gather('label', 'count', 1:2) %>%
  mutate(label=ifelse(label=='cont', 0, 1)) %>%
  mutate(actual_did_not_continue=label) %>%
  select(actual_did_not_continue, p_did_not_continue, count)

auc_data <- auc_data[rep(row.names(auc_data), auc_data$count), 1:2]

pred <- prediction(auc_data$p_did_not_continue, auc_data$actual_did_not_continue)

perf_nb <- performance(pred, measure='tpr', x.measure='fpr')

plot(perf_nb)

performance(pred, 'auc')
```


## To Do:
1. How to calculate AUC - tomorrow morning ✓
2. Split up models 3 + 4 stats by demographics ✓
3. Attendance -> factor ✓
4. Feature interactions? Francois
5. Cross validation and regularization? ?

6. Look at predicted values/coefs -> ask Jake how to make plots, etc.


